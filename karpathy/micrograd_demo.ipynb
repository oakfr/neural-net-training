{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oakfr/nn-training/blob/main/micrograd_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umWXC9EO1Dgn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9YNFAVL1JZw"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HViX5bY-1S3v"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Module:\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "class Neuron(Module):\n",
        "\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "        self.b = Value(0)\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def __call__(self, x):\n",
        "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
        "        return act.relu() if self.nonlin else act\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.w + [self.b]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n",
        "class Layer(Module):\n",
        "\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out = [n(x) for n in self.neurons]\n",
        "        return out[0] if len(out) == 1 else out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for n in self.neurons for p in n.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
        "\n",
        "class MLP(Module):\n",
        "\n",
        "    def __init__(self, nin, nouts):\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxnmHDeH1X0i"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1337)\n",
        "random.seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "3lpNBS5I1gOl",
        "outputId": "6fd02a1a-ad2f-48d5-dcc2-1623f89f5db5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f87d4a4a6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# make up a dataset\n",
        "\n",
        "from sklearn.datasets import make_moons, make_blobs\n",
        "X, y = make_moons(n_samples=100, noise=0.1)\n",
        "\n",
        "y = y*2 - 1 # make y be -1 or 1\n",
        "# visualize in 2D\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn5kKjr51hjJ",
        "outputId": "db74796b-9a27-49e3-e495-82b0cca176cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP of [Layer of [ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2)], Layer of [ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16)], Layer of [LinearNeuron(16)]]\n",
            "number of parameters 337\n"
          ]
        }
      ],
      "source": [
        "# initialize a model \n",
        "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
        "print(model)\n",
        "print(\"number of parameters\", len(model.parameters()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVPHaPDl1m_4",
        "outputId": "e4650ecb-793d-4bed-8436-80f870baf2a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=0.8958441028683222, grad=0) 0.5\n"
          ]
        }
      ],
      "source": [
        "# loss function\n",
        "def loss(batch_size=None):\n",
        "    \n",
        "    # inline DataLoader :)\n",
        "    if batch_size is None:\n",
        "        Xb, yb = X, y\n",
        "    else:\n",
        "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
        "        Xb, yb = X[ri], y[ri]\n",
        "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
        "    \n",
        "    # forward the model to get scores\n",
        "    scores = list(map(model, inputs))\n",
        "    \n",
        "    # svm \"max-margin\" loss\n",
        "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
        "    data_loss = sum(losses) * (1.0 / len(losses))\n",
        "    # L2 regularization\n",
        "    alpha = 1e-4\n",
        "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
        "    total_loss = data_loss + reg_loss\n",
        "    \n",
        "    # also get accuracy\n",
        "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
        "    return total_loss, sum(accuracy) / len(accuracy)\n",
        "\n",
        "total_loss, acc = loss()\n",
        "print(total_loss, acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVstB6T71r0g",
        "outputId": "3a7ee968-63f3-461f-d995-eac400c8aebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 loss 0.8958441028683222, accuracy 50.0%\n",
            "step 1 loss 1.723590533697202, accuracy 81.0%\n",
            "step 2 loss 0.742900631385113, accuracy 77.0%\n",
            "step 3 loss 0.7705641260584202, accuracy 82.0%\n",
            "step 4 loss 0.3692793385976538, accuracy 84.0%\n",
            "step 5 loss 0.31354548191852194, accuracy 86.0%\n",
            "step 6 loss 0.2814234349772434, accuracy 89.0%\n",
            "step 7 loss 0.2688873331398391, accuracy 91.0%\n",
            "step 8 loss 0.2567147286057416, accuracy 91.0%\n",
            "step 9 loss 0.27048625516379227, accuracy 91.0%\n",
            "step 10 loss 0.24507023853658036, accuracy 91.0%\n",
            "step 11 loss 0.25099055297915035, accuracy 92.0%\n",
            "step 12 loss 0.21560951851922946, accuracy 91.0%\n",
            "step 13 loss 0.23090378446402735, accuracy 93.0%\n",
            "step 14 loss 0.20152151227899445, accuracy 92.0%\n",
            "step 15 loss 0.22574506279282228, accuracy 93.0%\n",
            "step 16 loss 0.19447987596204103, accuracy 92.0%\n",
            "step 17 loss 0.21089496199246363, accuracy 93.0%\n",
            "step 18 loss 0.15983077356303604, accuracy 94.0%\n",
            "step 19 loss 0.18453748746883922, accuracy 93.0%\n",
            "step 20 loss 0.18977522856087634, accuracy 91.0%\n",
            "step 21 loss 0.19072704042579647, accuracy 93.0%\n",
            "step 22 loss 0.11733695088756484, accuracy 97.0%\n",
            "step 23 loss 0.12173524408232453, accuracy 95.0%\n",
            "step 24 loss 0.12615712612770452, accuracy 95.0%\n",
            "step 25 loss 0.16049097780801674, accuracy 95.0%\n",
            "step 26 loss 0.18747197705245805, accuracy 92.0%\n",
            "step 27 loss 0.16741837891059402, accuracy 95.0%\n",
            "step 28 loss 0.09586583491455393, accuracy 97.0%\n",
            "step 29 loss 0.08778783707420916, accuracy 96.0%\n",
            "step 30 loss 0.11731297569011845, accuracy 95.0%\n",
            "step 31 loss 0.09340146460619843, accuracy 97.0%\n",
            "step 32 loss 0.12454454903103447, accuracy 95.0%\n",
            "step 33 loss 0.0798400265277727, accuracy 97.0%\n",
            "step 34 loss 0.07727519232921669, accuracy 97.0%\n",
            "step 35 loss 0.07661250143094478, accuracy 98.0%\n",
            "step 36 loss 0.10610492379198365, accuracy 96.0%\n",
            "step 37 loss 0.0906280842926598, accuracy 99.0%\n",
            "step 38 loss 0.10671887043036926, accuracy 95.0%\n",
            "step 39 loss 0.052256599219758455, accuracy 98.0%\n",
            "step 40 loss 0.06016009895234468, accuracy 100.0%\n",
            "step 41 loss 0.08596724533333938, accuracy 96.0%\n",
            "step 42 loss 0.05112107943179598, accuracy 99.0%\n",
            "step 43 loss 0.05240142401642824, accuracy 97.0%\n",
            "step 44 loss 0.04530684179001576, accuracy 100.0%\n",
            "step 45 loss 0.07211073370655091, accuracy 97.0%\n",
            "step 46 loss 0.033342386513102305, accuracy 99.0%\n",
            "step 47 loss 0.031432227957511286, accuracy 100.0%\n",
            "step 48 loss 0.03658536747111499, accuracy 99.0%\n",
            "step 49 loss 0.04829139382390327, accuracy 99.0%\n",
            "step 50 loss 0.0987511476561962, accuracy 96.0%\n",
            "step 51 loss 0.05449063965875443, accuracy 99.0%\n",
            "step 52 loss 0.033926794357083206, accuracy 100.0%\n",
            "step 53 loss 0.05261517263568433, accuracy 97.0%\n",
            "step 54 loss 0.03250295251424912, accuracy 99.0%\n",
            "step 55 loss 0.028883273872078348, accuracy 100.0%\n",
            "step 56 loss 0.04139151104027228, accuracy 98.0%\n",
            "step 57 loss 0.018987407426128547, accuracy 100.0%\n",
            "step 58 loss 0.02523833523883733, accuracy 100.0%\n",
            "step 59 loss 0.020796565213419056, accuracy 100.0%\n",
            "step 60 loss 0.03259711157810217, accuracy 99.0%\n",
            "step 61 loss 0.017863351693480394, accuracy 100.0%\n",
            "step 62 loss 0.023008717832211603, accuracy 100.0%\n",
            "step 63 loss 0.022079325463581656, accuracy 100.0%\n",
            "step 64 loss 0.029432917853529528, accuracy 99.0%\n",
            "step 65 loss 0.01625151464409207, accuracy 100.0%\n",
            "step 66 loss 0.02846853448326431, accuracy 99.0%\n",
            "step 67 loss 0.013994365546208703, accuracy 100.0%\n",
            "step 68 loss 0.015552344843651632, accuracy 100.0%\n",
            "step 69 loss 0.03389119946160154, accuracy 99.0%\n",
            "step 70 loss 0.014229870065926888, accuracy 100.0%\n",
            "step 71 loss 0.01325528158328547, accuracy 100.0%\n",
            "step 72 loss 0.012300277590022037, accuracy 100.0%\n",
            "step 73 loss 0.012676052498356172, accuracy 100.0%\n",
            "step 74 loss 0.020593811955954645, accuracy 100.0%\n",
            "step 75 loss 0.011845398205364377, accuracy 100.0%\n",
            "step 76 loss 0.01601269747288338, accuracy 100.0%\n",
            "step 77 loss 0.025458360239221975, accuracy 100.0%\n",
            "step 78 loss 0.014382930289661837, accuracy 100.0%\n",
            "step 79 loss 0.011698962425817921, accuracy 100.0%\n",
            "step 80 loss 0.012318500800515903, accuracy 100.0%\n",
            "step 81 loss 0.014121117031464158, accuracy 100.0%\n",
            "step 82 loss 0.011664591962446154, accuracy 100.0%\n",
            "step 83 loss 0.011589314549188772, accuracy 100.0%\n",
            "step 84 loss 0.010990299347735228, accuracy 100.0%\n",
            "step 85 loss 0.010989226720691612, accuracy 100.0%\n",
            "step 86 loss 0.010988193757655071, accuracy 100.0%\n",
            "step 87 loss 0.010987200447388707, accuracy 100.0%\n",
            "step 88 loss 0.010986246779084925, accuracy 100.0%\n",
            "step 89 loss 0.010985332742365274, accuracy 100.0%\n",
            "step 90 loss 0.010984458327280176, accuracy 100.0%\n",
            "step 91 loss 0.010983623524308863, accuracy 100.0%\n",
            "step 92 loss 0.010982828324359073, accuracy 100.0%\n",
            "step 93 loss 0.010982072718767001, accuracy 100.0%\n",
            "step 94 loss 0.010981356699297043, accuracy 100.0%\n",
            "step 95 loss 0.010980680258141723, accuracy 100.0%\n",
            "step 96 loss 0.010980043387921508, accuracy 100.0%\n",
            "step 97 loss 0.010979446081684675, accuracy 100.0%\n",
            "step 98 loss 0.010978888332907229, accuracy 100.0%\n",
            "step 99 loss 0.010978370135492719, accuracy 100.0%\n",
            "step 100 loss 0.010977891483772166, accuracy 100.0%\n",
            "step 101 loss 0.010977452372503976, accuracy 100.0%\n",
            "step 102 loss 0.010977052796873792, accuracy 100.0%\n",
            "step 103 loss 0.01097669275249444, accuracy 100.0%\n",
            "step 104 loss 0.010976372235405861, accuracy 100.0%\n",
            "step 105 loss 0.010976091242075004, accuracy 100.0%\n",
            "step 106 loss 0.010975849769395783, accuracy 100.0%\n",
            "step 107 loss 0.010975647814689023, accuracy 100.0%\n",
            "step 108 loss 0.010975485375702384, accuracy 100.0%\n",
            "step 109 loss 0.010975362450610378, accuracy 100.0%\n",
            "step 110 loss 0.010975279038014235, accuracy 100.0%\n",
            "step 111 loss 0.010975235136941987, accuracy 100.0%\n",
            "step 112 loss 0.010975230746848362, accuracy 100.0%\n",
            "step 113 loss 0.010975265867614842, accuracy 100.0%\n",
            "step 114 loss 0.010975340499549627, accuracy 100.0%\n",
            "step 115 loss 0.010975454643387603, accuracy 100.0%\n",
            "step 116 loss 0.010975608300290396, accuracy 100.0%\n",
            "step 117 loss 0.010975801471846432, accuracy 100.0%\n",
            "step 118 loss 0.010976034160070881, accuracy 100.0%\n",
            "step 119 loss 0.010976306367405729, accuracy 100.0%\n",
            "step 120 loss 0.010976618096719813, accuracy 100.0%\n",
            "step 121 loss 0.010976969351308923, accuracy 100.0%\n",
            "step 122 loss 0.010977360134895772, accuracy 100.0%\n",
            "step 123 loss 0.010977790451630129, accuracy 100.0%\n",
            "step 124 loss 0.010978260306088849, accuracy 100.0%\n",
            "step 125 loss 0.01097876970327599, accuracy 100.0%\n",
            "step 126 loss 0.010979318648622878, accuracy 100.0%\n",
            "step 127 loss 0.01097990714798824, accuracy 100.0%\n",
            "step 128 loss 0.01098053520765823, accuracy 100.0%\n",
            "step 129 loss 0.010981202834346624, accuracy 100.0%\n",
            "step 130 loss 0.010981910035194909, accuracy 100.0%\n",
            "step 131 loss 0.010982656817772394, accuracy 100.0%\n",
            "step 132 loss 0.01098344319007635, accuracy 100.0%\n",
            "step 133 loss 0.010984269160532191, accuracy 100.0%\n",
            "step 134 loss 0.010985134737993587, accuracy 100.0%\n",
            "step 135 loss 0.010986039931742603, accuracy 100.0%\n",
            "step 136 loss 0.01098698475148992, accuracy 100.0%\n",
            "step 137 loss 0.01098796920737497, accuracy 100.0%\n",
            "step 138 loss 0.010988993309966133, accuracy 100.0%\n",
            "step 139 loss 0.010990057070260918, accuracy 100.0%\n",
            "step 140 loss 0.010991160499686141, accuracy 100.0%\n",
            "step 141 loss 0.010992303610098229, accuracy 100.0%\n",
            "step 142 loss 0.010993486413783228, accuracy 100.0%\n",
            "step 143 loss 0.010994708923457271, accuracy 100.0%\n",
            "step 144 loss 0.010995971152266602, accuracy 100.0%\n",
            "step 145 loss 0.010997273113787957, accuracy 100.0%\n",
            "step 146 loss 0.010998614822028685, accuracy 100.0%\n",
            "step 147 loss 0.010999996291427117, accuracy 100.0%\n",
            "step 148 loss 0.011001417536852704, accuracy 100.0%\n",
            "step 149 loss 0.011002878573606416, accuracy 100.0%\n",
            "step 150 loss 0.011004379417420888, accuracy 100.0%\n",
            "step 151 loss 0.011005920084460784, accuracy 100.0%\n",
            "step 152 loss 0.011007500591323067, accuracy 100.0%\n",
            "step 153 loss 0.0110091209550373, accuracy 100.0%\n",
            "step 154 loss 0.01101078119306594, accuracy 100.0%\n",
            "step 155 loss 0.011012481323304641, accuracy 100.0%\n",
            "step 156 loss 0.011014221364082616, accuracy 100.0%\n",
            "step 157 loss 0.01101600133416294, accuracy 100.0%\n",
            "step 158 loss 0.011017821252742883, accuracy 100.0%\n",
            "step 159 loss 0.011019681139454245, accuracy 100.0%\n",
            "step 160 loss 0.011021581014363771, accuracy 100.0%\n",
            "step 161 loss 0.011023520897973422, accuracy 100.0%\n",
            "step 162 loss 0.011025500811220801, accuracy 100.0%\n",
            "step 163 loss 0.011027520775479565, accuracy 100.0%\n",
            "step 164 loss 0.011029580812559657, accuracy 100.0%\n",
            "step 165 loss 0.011031680944707894, accuracy 100.0%\n",
            "step 166 loss 0.011033821194608261, accuracy 100.0%\n",
            "step 167 loss 0.011036001585382302, accuracy 100.0%\n",
            "step 168 loss 0.011038222140589582, accuracy 100.0%\n",
            "step 169 loss 0.01104048288422812, accuracy 100.0%\n",
            "step 170 loss 0.011042783840734791, accuracy 100.0%\n",
            "step 171 loss 0.011045125034985745, accuracy 100.0%\n",
            "step 172 loss 0.011047506492296914, accuracy 100.0%\n",
            "step 173 loss 0.011049928238424427, accuracy 100.0%\n",
            "step 174 loss 0.011052390299565139, accuracy 100.0%\n",
            "step 175 loss 0.011054892702356935, accuracy 100.0%\n",
            "step 176 loss 0.011057435473879423, accuracy 100.0%\n",
            "step 177 loss 0.011060018641654325, accuracy 100.0%\n",
            "step 178 loss 0.011062642233645892, accuracy 100.0%\n",
            "step 179 loss 0.011065306278261593, accuracy 100.0%\n",
            "step 180 loss 0.011068010804352438, accuracy 100.0%\n",
            "step 181 loss 0.011070755841213652, accuracy 100.0%\n",
            "step 182 loss 0.011073541418585102, accuracy 100.0%\n",
            "step 183 loss 0.011076367566651873, accuracy 100.0%\n",
            "step 184 loss 0.011079234316044814, accuracy 100.0%\n",
            "step 185 loss 0.011082141697841066, accuracy 100.0%\n",
            "step 186 loss 0.011085089743564697, accuracy 100.0%\n",
            "step 187 loss 0.011088078485187182, accuracy 100.0%\n",
            "step 188 loss 0.011091107955127993, accuracy 100.0%\n",
            "step 189 loss 0.011094178186255268, accuracy 100.0%\n",
            "step 190 loss 0.01109728921188631, accuracy 100.0%\n",
            "step 191 loss 0.01110044106578822, accuracy 100.0%\n",
            "step 192 loss 0.011103633782178551, accuracy 100.0%\n",
            "step 193 loss 0.011106867395725847, accuracy 100.0%\n",
            "step 194 loss 0.01111014194155034, accuracy 100.0%\n",
            "step 195 loss 0.011113457455224575, accuracy 100.0%\n",
            "step 196 loss 0.011116813972773995, accuracy 100.0%\n",
            "step 197 loss 0.011120211530677663, accuracy 100.0%\n",
            "step 198 loss 0.011123650165868908, accuracy 100.0%\n",
            "step 199 loss 0.011127129915735955, accuracy 100.0%\n",
            "step 200 loss 0.01113065081812264, accuracy 100.0%\n",
            "step 201 loss 0.011134212911329107, accuracy 100.0%\n",
            "step 202 loss 0.011137816234112446, accuracy 100.0%\n",
            "step 203 loss 0.011141460825687444, accuracy 100.0%\n",
            "step 204 loss 0.011145146725727318, accuracy 100.0%\n",
            "step 205 loss 0.011148873974364341, accuracy 100.0%\n",
            "step 206 loss 0.011152642612190664, accuracy 100.0%\n",
            "step 207 loss 0.01115645268025901, accuracy 100.0%\n",
            "step 208 loss 0.011160304220083449, accuracy 100.0%\n",
            "step 209 loss 0.011164197273640073, accuracy 100.0%\n",
            "step 210 loss 0.011168131883367894, accuracy 100.0%\n",
            "step 211 loss 0.011172108092169467, accuracy 100.0%\n",
            "step 212 loss 0.01117612594341176, accuracy 100.0%\n",
            "step 213 loss 0.011180185480926945, accuracy 100.0%\n",
            "step 214 loss 0.01118428674901315, accuracy 100.0%\n",
            "step 215 loss 0.011188429792435286, accuracy 100.0%\n",
            "step 216 loss 0.011192614656425862, accuracy 100.0%\n",
            "step 217 loss 0.011196841386685797, accuracy 100.0%\n",
            "step 218 loss 0.011201110029385295, accuracy 100.0%\n",
            "step 219 loss 0.011205420631164604, accuracy 100.0%\n",
            "step 220 loss 0.011209773239134935, accuracy 100.0%\n",
            "step 221 loss 0.011214167900879334, accuracy 100.0%\n",
            "step 222 loss 0.011218604664453439, accuracy 100.0%\n",
            "step 223 loss 0.011223083578386509, accuracy 100.0%\n",
            "step 224 loss 0.011227604691682146, accuracy 100.0%\n",
            "step 225 loss 0.011232168053819332, accuracy 100.0%\n",
            "step 226 loss 0.011236773714753262, accuracy 100.0%\n",
            "step 227 loss 0.011241421724916247, accuracy 100.0%\n",
            "step 228 loss 0.011246112135218663, accuracy 100.0%\n",
            "step 229 loss 0.011250844997049854, accuracy 100.0%\n",
            "step 230 loss 0.011255620362279095, accuracy 100.0%\n",
            "step 231 loss 0.011260438283256532, accuracy 100.0%\n",
            "step 232 loss 0.011265298812814148, accuracy 100.0%\n",
            "step 233 loss 0.011270202004266695, accuracy 100.0%\n",
            "step 234 loss 0.011275147911412658, accuracy 100.0%\n",
            "step 235 loss 0.011280136588535349, accuracy 100.0%\n",
            "step 236 loss 0.011285168090403735, accuracy 100.0%\n",
            "step 237 loss 0.011290242472273605, accuracy 100.0%\n",
            "step 238 loss 0.011295359789888448, accuracy 100.0%\n",
            "step 239 loss 0.01130052009948053, accuracy 100.0%\n",
            "step 240 loss 0.01130572345777194, accuracy 100.0%\n",
            "step 241 loss 0.011310969921975594, accuracy 100.0%\n",
            "step 242 loss 0.011316259549796332, accuracy 100.0%\n",
            "step 243 loss 0.011321592399431885, accuracy 100.0%\n",
            "step 244 loss 0.011326968529574048, accuracy 100.0%\n",
            "step 245 loss 0.011332387999409674, accuracy 100.0%\n",
            "step 246 loss 0.011337850868621826, accuracy 100.0%\n",
            "step 247 loss 0.011343357197390783, accuracy 100.0%\n",
            "step 248 loss 0.011348907046395294, accuracy 100.0%\n",
            "step 249 loss 0.0113545004768135, accuracy 100.0%\n",
            "step 250 loss 0.011360137550324202, accuracy 100.0%\n",
            "step 251 loss 0.011365818329107965, accuracy 100.0%\n",
            "step 252 loss 0.011371542875848219, accuracy 100.0%\n",
            "step 253 loss 0.01137731125373242, accuracy 100.0%\n",
            "step 254 loss 0.011383123526453275, accuracy 100.0%\n",
            "step 255 loss 0.011388979758209858, accuracy 100.0%\n",
            "step 256 loss 0.011394880013708763, accuracy 100.0%\n",
            "step 257 loss 0.011400824358165362, accuracy 100.0%\n",
            "step 258 loss 0.011406812857304983, accuracy 100.0%\n",
            "step 259 loss 0.011412845577364095, accuracy 100.0%\n",
            "step 260 loss 0.011418922585091579, accuracy 100.0%\n",
            "step 261 loss 0.011425043947749885, accuracy 100.0%\n",
            "step 262 loss 0.011431209733116336, accuracy 100.0%\n",
            "step 263 loss 0.011437420009484386, accuracy 100.0%\n",
            "step 264 loss 0.011443674845664817, accuracy 100.0%\n",
            "step 265 loss 0.011449974310987042, accuracy 100.0%\n",
            "step 266 loss 0.011456318475300398, accuracy 100.0%\n",
            "step 267 loss 0.011462707408975443, accuracy 100.0%\n",
            "step 268 loss 0.011469141182905224, accuracy 100.0%\n",
            "step 269 loss 0.01147561986850659, accuracy 100.0%\n",
            "step 270 loss 0.011482143537721526, accuracy 100.0%\n",
            "step 271 loss 0.011488712263018517, accuracy 100.0%\n",
            "step 272 loss 0.011495326117393826, accuracy 100.0%\n",
            "step 273 loss 0.011501985174372873, accuracy 100.0%\n",
            "step 274 loss 0.01150868950801161, accuracy 100.0%\n",
            "step 275 loss 0.01151543919289786, accuracy 100.0%\n",
            "step 276 loss 0.011522234304152768, accuracy 100.0%\n",
            "step 277 loss 0.011529074917432106, accuracy 100.0%\n",
            "step 278 loss 0.01153596110892771, accuracy 100.0%\n",
            "step 279 loss 0.011542892955368934, accuracy 100.0%\n",
            "step 280 loss 0.011549870534023995, accuracy 100.0%\n",
            "step 281 loss 0.011556893922701525, accuracy 100.0%\n",
            "step 282 loss 0.011563963199751875, accuracy 100.0%\n",
            "step 283 loss 0.011571078444068657, accuracy 100.0%\n",
            "step 284 loss 0.011578239735090205, accuracy 100.0%\n",
            "step 285 loss 0.011585447152801048, accuracy 100.0%\n",
            "step 286 loss 0.011592700777733368, accuracy 100.0%\n",
            "step 287 loss 0.011600000690968547, accuracy 100.0%\n",
            "step 288 loss 0.011607346974138632, accuracy 100.0%\n",
            "step 289 loss 0.011614739709427889, accuracy 100.0%\n",
            "step 290 loss 0.011622178979574297, accuracy 100.0%\n",
            "step 291 loss 0.011629664867871143, accuracy 100.0%\n",
            "step 292 loss 0.01163719745816855, accuracy 100.0%\n",
            "step 293 loss 0.011644776834874986, accuracy 100.0%\n",
            "step 294 loss 0.01165240308295894, accuracy 100.0%\n",
            "step 295 loss 0.011660076287950455, accuracy 100.0%\n",
            "step 296 loss 0.011667796535942677, accuracy 100.0%\n",
            "step 297 loss 0.011675563913593612, accuracy 100.0%\n",
            "step 298 loss 0.01168337850812756, accuracy 100.0%\n",
            "step 299 loss 0.011691240407336899, accuracy 100.0%\n"
          ]
        }
      ],
      "source": [
        "# optimization\n",
        "for k in range(300):\n",
        "    \n",
        "    # forward\n",
        "    total_loss, acc = loss()\n",
        "    \n",
        "    # backward\n",
        "    model.zero_grad()\n",
        "    total_loss.backward()\n",
        "    \n",
        "    # update (sgd)\n",
        "    learning_rate = (1.0 - 0.9*k/100)/1\n",
        "    for p in model.parameters():\n",
        "        p.data -= learning_rate * p.grad\n",
        "    \n",
        "    if k % 1 == 0:\n",
        "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59YyROCj2QmX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM26ARFGJeaTVos7KThq19X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}